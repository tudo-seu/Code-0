{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "# Load the labeled datasets\n",
    "df_labeled_1 = pd.read_csv('../data_new/supervised-dropped/AB3 - Main 3L.csv')\n",
    "df_labeled_2 = pd.read_csv('../data_new/supervised-dropped/BS1 - Main 1L.csv')\n",
    "df_labeled_3 = pd.read_csv('../data_new/supervised-dropped/BS4 - Main 4R.csv')\n",
    "df_labeled_4 = pd.read_csv('../data_new/supervised-dropped/Sub - Feeder F02.csv')\n",
    "\n",
    "# Train a Random Forest model on the first labeled dataset\n",
    "df = df_labeled_1.copy()\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['weekday'] = df['time'].dt.weekday\n",
    "df = df.drop('time', axis=1)\n",
    "\n",
    "df['Measure'] = LabelEncoder().fit_transform(df['Measure'])\n",
    "scaler = StandardScaler()\n",
    "df['kWh'] = scaler.fit_transform(df[['kWh']])\n",
    "df['label'] = LabelEncoder().fit_transform(df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest model on the second labeled dataset\n",
    "df = df_labeled_2.copy()\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['weekday'] = df['time'].dt.weekday\n",
    "df = df.drop('time', axis=1)\n",
    "\n",
    "df['Measure'] = LabelEncoder().fit_transform(df['Measure'])\n",
    "scaler = StandardScaler()\n",
    "df['kWh'] = scaler.fit_transform(df[['kWh']])\n",
    "df['label'] = LabelEncoder().fit_transform(df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest model on the third labeled dataset\n",
    "df = df_labeled_3.copy()\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['weekday'] = df['time'].dt.weekday\n",
    "df = df.drop('time', axis=1)\n",
    "\n",
    "df['Measure'] = LabelEncoder().fit_transform(df['Measure'])\n",
    "scaler = StandardScaler()\n",
    "df['kWh'] = scaler.fit_transform(df[['kWh']])\n",
    "df['label'] = LabelEncoder().fit_transform(df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "model3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Save the models\n",
    "with open('../models/semi_supervised_rfc_model1.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load the new dataframe to be predicted\n",
    "new_df = pd.read_csv('../data_new/supervised-dropped/BS1 - Main 1L.csv') #  pd.read_csv('../data_new/holdout_v3.csv')\n",
    "\n",
    "# Preprocess the new data\n",
    "new_df['time'] = pd.to_datetime(new_df['time'])\n",
    "new_df['year'] = new_df['time'].dt.year\n",
    "new_df['month'] = new_df['time'].dt.month\n",
    "new_df['day'] = new_df['time'].dt.day\n",
    "new_df['hour'] = new_df['time'].dt.hour\n",
    "new_df['weekday'] = new_df['time'].dt.weekday\n",
    "new_df = new_df.drop('time', axis=1)\n",
    "\n",
    "new_df['Measure'] = LabelEncoder().fit_transform(new_df['Measure'])\n",
    "scaler = StandardScaler()\n",
    "new_df['kWh'] = scaler.fit_transform(new_df[['kWh']])\n",
    "\n",
    "# Predict the labels for the new data using the trained model\n",
    "new_preds = model.predict(new_df.drop('label', axis=1))\n",
    "\n",
    "# Print the predicted labels\n",
    "print(new_preds)\n",
    "# print(label_encoder.inverse_transform(new_df['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9236479321314952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(new_df['label'])\n",
    "\n",
    "# Calculate the accuracy score of the model on the new data\n",
    "accuracy = accuracy_score(y_encoded, new_preds)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m df_unlabeled \u001b[39m=\u001b[39m df_unlabeled\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m df_unlabeled[\u001b[39m'\u001b[39m\u001b[39mMeasure\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit_transform(df_unlabeled[\u001b[39m'\u001b[39m\u001b[39mMeasure\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 46\u001b[0m df_unlabeled[\u001b[39m'\u001b[39m\u001b[39mkWh\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(df_unlabeled[[\u001b[39m'\u001b[39;49m\u001b[39mkWh\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n\u001b[1;32m     48\u001b[0m \u001b[39m# Train a semi-supervised learning model\u001b[39;00m\n\u001b[1;32m     49\u001b[0m model \u001b[39m=\u001b[39m LabelPropagation(kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mknn\u001b[39m\u001b[39m'\u001b[39m, n_neighbors\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    989\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    991\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[0;32m--> 992\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    993\u001b[0m     X,\n\u001b[1;32m    994\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    995\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    996\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    997\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    998\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    999\u001b[0m )\n\u001b[1;32m   1001\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[1;32m   1002\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    935\u001b[0m         )\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "import pickle\n",
    "\n",
    "# Load all data\n",
    "df_labeled_1 = pd.read_csv('../data_new/supervised-dropped/AB3 - Main 3L.csv')\n",
    "df_labeled_2 = pd.read_csv('../data_new/supervised-dropped/BS1 - Main 1L.csv')\n",
    "df_labeled_3 = pd.read_csv('../data_new/supervised-dropped/BS4 - Main 4R.csv')\n",
    "df_labeled_4 = pd.read_csv('../data_new/supervised-dropped/Sub - Feeder F02.csv')\n",
    "df_unlabeled = pd.read_csv('../data_new/holdout_v3.csv')\n",
    "\n",
    "# Concatenate labeled and unlabeled data into a single DataFrame\n",
    "df = pd.concat([df_labeled_1, df_labeled_2, df_labeled_3, df_labeled_4, df_unlabeled], ignore_index=True)\n",
    "\n",
    "# Split the data into labeled and unlabeled sets\n",
    "labeled_mask = df['label'].notnull()\n",
    "df_labeled = df[labeled_mask].copy()\n",
    "df_unlabeled = df[~labeled_mask].copy()\n",
    "\n",
    "# Preprocess the data\n",
    "df_labeled['time'] = pd.to_datetime(df_labeled['time'])\n",
    "df_labeled['year'] = df_labeled['time'].dt.year\n",
    "df_labeled['month'] = df_labeled['time'].dt.month\n",
    "df_labeled['day'] = df_labeled['time'].dt.day\n",
    "df_labeled['hour'] = df_labeled['time'].dt.hour\n",
    "df_labeled['weekday'] = df_labeled['time'].dt.weekday\n",
    "df_labeled = df_labeled.drop('time', axis=1)\n",
    "\n",
    "df_labeled['Measure'] = LabelEncoder().fit_transform(df_labeled['Measure'])\n",
    "scaler = StandardScaler()\n",
    "df_labeled['kWh'] = scaler.fit_transform(df_labeled[['kWh']])\n",
    "df_labeled['label'] = LabelEncoder().fit_transform(df_labeled['label'])\n",
    "\n",
    "df_unlabeled['time'] = pd.to_datetime(df_unlabeled['time'])\n",
    "df_unlabeled['year'] = df_unlabeled['time'].dt.year\n",
    "df_unlabeled['month'] = df_unlabeled['time'].dt.month\n",
    "df_unlabeled['day'] = df_unlabeled['time'].dt.day\n",
    "df_unlabeled['hour'] = df_unlabeled['time'].dt.hour\n",
    "df_unlabeled['weekday'] = df_unlabeled['time'].dt.weekday\n",
    "df_unlabeled = df_unlabeled.drop('time', axis=1)\n",
    "\n",
    "df_unlabeled['Measure'] = LabelEncoder().fit_transform(df_unlabeled['Measure'])\n",
    "df_unlabeled['kWh'] = scaler.transform(df_unlabeled[['kWh']])\n",
    "\n",
    "# Train a semi-supervised learning model\n",
    "model = LabelPropagation(kernel='knn', n_neighbors=7)\n",
    "X_labeled = df_labeled.drop('label', axis=1)\n",
    "y_labeled = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
