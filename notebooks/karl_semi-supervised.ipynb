{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "# Load the labeled datasets\n",
    "df_labeled_1 = pd.read_csv('../data_new/supervised-dropped/AB3 - Main 3L.csv')\n",
    "df_labeled_2 = pd.read_csv('../data_new/supervised-dropped/BS1 - Main 1L.csv')\n",
    "df_labeled_3 = pd.read_csv('../data_new/supervised-dropped/BS4 - Main 4R.csv')\n",
    "df_labeled_4 = pd.read_csv('../data_new/supervised-dropped/Sub - Feeder F02.csv')\n",
    "\n",
    "# Train a Random Forest model on the first labeled dataset\n",
    "df = df_labeled_1.copy()\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['weekday'] = df['time'].dt.weekday\n",
    "df = df.drop('time', axis=1)\n",
    "\n",
    "df['Measure'] = LabelEncoder().fit_transform(df['Measure'])\n",
    "scaler = StandardScaler()\n",
    "df['kWh'] = scaler.fit_transform(df[['kWh']])\n",
    "df['label'] = LabelEncoder().fit_transform(df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest model on the second labeled dataset\n",
    "df = df_labeled_2.copy()\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['weekday'] = df['time'].dt.weekday\n",
    "df = df.drop('time', axis=1)\n",
    "\n",
    "df['Measure'] = LabelEncoder().fit_transform(df['Measure'])\n",
    "scaler = StandardScaler()\n",
    "df['kWh'] = scaler.fit_transform(df[['kWh']])\n",
    "df['label'] = LabelEncoder().fit_transform(df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest model on the third labeled dataset\n",
    "df = df_labeled_3.copy()\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['weekday'] = df['time'].dt.weekday\n",
    "df = df.drop('time', axis=1)\n",
    "\n",
    "df['Measure'] = LabelEncoder().fit_transform(df['Measure'])\n",
    "scaler = StandardScaler()\n",
    "df['kWh'] = scaler.fit_transform(df[['kWh']])\n",
    "df['label'] = LabelEncoder().fit_transform(df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "model3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Save the models\n",
    "with open('../models/semi_supervised_rfc_model1.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load the new dataframe to be predicted\n",
    "new_df = pd.read_csv('../data_new/supervised-dropped/BS1 - Main 1L.csv') #  pd.read_csv('../data_new/holdout_v3.csv')\n",
    "\n",
    "# Preprocess the new data\n",
    "new_df['time'] = pd.to_datetime(new_df['time'])\n",
    "new_df['year'] = new_df['time'].dt.year\n",
    "new_df['month'] = new_df['time'].dt.month\n",
    "new_df['day'] = new_df['time'].dt.day\n",
    "new_df['hour'] = new_df['time'].dt.hour\n",
    "new_df['weekday'] = new_df['time'].dt.weekday\n",
    "new_df = new_df.drop('time', axis=1)\n",
    "\n",
    "new_df['Measure'] = LabelEncoder().fit_transform(new_df['Measure'])\n",
    "scaler = StandardScaler()\n",
    "new_df['kWh'] = scaler.fit_transform(new_df[['kWh']])\n",
    "\n",
    "# Predict the labels for the new data using the trained model\n",
    "new_preds = model.predict(new_df.drop('label', axis=1))\n",
    "\n",
    "# Print the predicted labels\n",
    "print(new_preds)\n",
    "# print(label_encoder.inverse_transform(new_df['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9236479321314952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(new_df['label'])\n",
    "\n",
    "# Calculate the accuracy score of the model on the new data\n",
    "accuracy = accuracy_score(y_encoded, new_preds)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m df_unlabeled \u001b[39m=\u001b[39m df_unlabeled\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m df_unlabeled[\u001b[39m'\u001b[39m\u001b[39mMeasure\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit_transform(df_unlabeled[\u001b[39m'\u001b[39m\u001b[39mMeasure\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 46\u001b[0m df_unlabeled[\u001b[39m'\u001b[39m\u001b[39mkWh\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(df_unlabeled[[\u001b[39m'\u001b[39;49m\u001b[39mkWh\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n\u001b[1;32m     48\u001b[0m \u001b[39m# Train a semi-supervised learning model\u001b[39;00m\n\u001b[1;32m     49\u001b[0m model \u001b[39m=\u001b[39m LabelPropagation(kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mknn\u001b[39m\u001b[39m'\u001b[39m, n_neighbors\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    989\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    991\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[0;32m--> 992\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    993\u001b[0m     X,\n\u001b[1;32m    994\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    995\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    996\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    997\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    998\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    999\u001b[0m )\n\u001b[1;32m   1001\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[1;32m   1002\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    935\u001b[0m         )\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "import pickle\n",
    "\n",
    "# Load all data\n",
    "df_labeled_1 = pd.read_csv('../data_new/supervised-dropped/AB3 - Main 3L.csv')\n",
    "df_labeled_2 = pd.read_csv('../data_new/supervised-dropped/BS1 - Main 1L.csv')\n",
    "df_labeled_3 = pd.read_csv('../data_new/supervised-dropped/BS4 - Main 4R.csv')\n",
    "df_labeled_4 = pd.read_csv('../data_new/supervised-dropped/Sub - Feeder F02.csv')\n",
    "df_unlabeled = pd.read_csv('../data_new/holdout_v3.csv')\n",
    "\n",
    "# Concatenate labeled and unlabeled data into a single DataFrame\n",
    "df = pd.concat([df_labeled_1, df_labeled_2, df_labeled_3, df_labeled_4, df_unlabeled], ignore_index=True)\n",
    "\n",
    "# Split the data into labeled and unlabeled sets\n",
    "labeled_mask = df['label'].notnull()\n",
    "df_labeled = df[labeled_mask].copy()\n",
    "df_unlabeled = df[~labeled_mask].copy()\n",
    "\n",
    "# Preprocess the data\n",
    "df_labeled['time'] = pd.to_datetime(df_labeled['time'])\n",
    "df_labeled['year'] = df_labeled['time'].dt.year\n",
    "df_labeled['month'] = df_labeled['time'].dt.month\n",
    "df_labeled['day'] = df_labeled['time'].dt.day\n",
    "df_labeled['hour'] = df_labeled['time'].dt.hour\n",
    "df_labeled['weekday'] = df_labeled['time'].dt.weekday\n",
    "df_labeled = df_labeled.drop('time', axis=1)\n",
    "\n",
    "df_labeled['Measure'] = LabelEncoder().fit_transform(df_labeled['Measure'])\n",
    "scaler = StandardScaler()\n",
    "df_labeled['kWh'] = scaler.fit_transform(df_labeled[['kWh']])\n",
    "df_labeled['label'] = LabelEncoder().fit_transform(df_labeled['label'])\n",
    "\n",
    "df_unlabeled['time'] = pd.to_datetime(df_unlabeled['time'])\n",
    "df_unlabeled['year'] = df_unlabeled['time'].dt.year\n",
    "df_unlabeled['month'] = df_unlabeled['time'].dt.month\n",
    "df_unlabeled['day'] = df_unlabeled['time'].dt.day\n",
    "df_unlabeled['hour'] = df_unlabeled['time'].dt.hour\n",
    "df_unlabeled['weekday'] = df_unlabeled['time'].dt.weekday\n",
    "df_unlabeled = df_unlabeled.drop('time', axis=1)\n",
    "\n",
    "df_unlabeled['Measure'] = LabelEncoder().fit_transform(df_unlabeled['Measure'])\n",
    "df_unlabeled['kWh'] = scaler.transform(df_unlabeled[['kWh']])\n",
    "\n",
    "# Train a semi-supervised learning model\n",
    "model = LabelPropagation(kernel='knn', n_neighbors=7)\n",
    "X_labeled = df_labeled.drop('label', axis=1)\n",
    "y_labeled = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sktime in /home/operation/.local/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy<1.25,>=1.21.0 in /home/operation/.local/lib/python3.10/site-packages (from sktime) (1.23.5)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.1.0 in /home/operation/.local/lib/python3.10/site-packages (from sktime) (1.5.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /home/operation/.local/lib/python3.10/site-packages (from sktime) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn<1.3.0,>=0.24.0 in /home/operation/.local/lib/python3.10/site-packages (from sktime) (1.2.2)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /home/operation/.local/lib/python3.10/site-packages (from sktime) (1.2.13)\n",
      "Requirement already satisfied: scikit-base<0.5.0 in /home/operation/.local/lib/python3.10/site-packages (from sktime) (0.4.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/operation/.local/lib/python3.10/site-packages (from deprecated>=1.2.13->sktime) (1.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/operation/.local/lib/python3.10/site-packages (from pandas<2.0.0,>=1.1.0->sktime) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/operation/miniconda3/envs/ds/lib/python3.10/site-packages (from pandas<2.0.0,>=1.1.0->sktime) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/operation/.local/lib/python3.10/site-packages (from scikit-learn<1.3.0,>=0.24.0->sktime) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/operation/.local/lib/python3.10/site-packages (from scikit-learn<1.3.0,>=0.24.0->sktime) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/operation/miniconda3/envs/ds/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.1.0->sktime) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sktime.transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msktime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseries_as_features\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompose\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureUnion\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msktime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseries_as_features\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msegment\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomIntervalSegmenter\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msktime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseries_as_features\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarize\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomIntervalFeatureExtractor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sktime.transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sktime.transformers.series_as_features.compose import FeatureUnion\n",
    "from sktime.transformers.series_as_features.segment import RandomIntervalSegmenter\n",
    "from sktime.transformers.series_as_features.summarize import RandomIntervalFeatureExtractor\n",
    "from sktime.classification.compose import HIVECOTEV1\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../data_new/supervised-dropped/AB3 - Main 3L.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Split data into training and test sets\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(df\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mPhase\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), df[\u001b[39m\"\u001b[39m\u001b[39mPhase\u001b[39m\u001b[39m\"\u001b[39m], test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Define preprocessing pipeline\u001b[39;00m\n\u001b[1;32m      8\u001b[0m preprocess \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m      9\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39msegment\u001b[39m\u001b[39m\"\u001b[39m, RandomIntervalSegmenter(n_intervals\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m\"\u001b[39m)),\n\u001b[1;32m     10\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m\"\u001b[39m, FeatureUnion([\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ]))\n\u001b[1;32m     17\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data_new/supervised-dropped/AB3 - Main 3L.csv\")\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(\"Phase\", axis=1), df[\"Phase\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "preprocess = Pipeline([\n",
    "    (\"segment\", RandomIntervalSegmenter(n_intervals=\"sqrt\")),\n",
    "    (\"transform\", FeatureUnion([\n",
    "        (\"mean\", RandomIntervalFeatureExtractor(feature=\"mean\")),\n",
    "        (\"std\", RandomIntervalFeatureExtractor(feature=\"std\")),\n",
    "        (\"min\", RandomIntervalFeatureExtractor(feature=\"min\")),\n",
    "        (\"max\", RandomIntervalFeatureExtractor(feature=\"max\")),\n",
    "        (\"fft\", RandomIntervalFeatureExtractor(feature=\"fft\", n_components=10))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Define HIVE-COTE ensemble\n",
    "ensemble = HIVECOTEV1(\n",
    "    estimators=[\n",
    "        (\"transformed_features\", preprocess),\n",
    "        (\"shallow_forest\", \"shallow-forest\", {\"n_estimators\": 200}),\n",
    "        (\"k_neighbors\", \"k-nn\", {\"n_neighbors\": 1}),\n",
    "        (\"random_subspace\", \"random-subspace\", {\"n_estimators\": 200}),\n",
    "        (\"boss\", \"boss\", {\"max_ensemble_size\": 5, \"n_parameter_samples\": 100}),\n",
    "        (\"st\", \"shapelet_transform\", {\"max_shapelets_to_store_per_class\": 100}),\n",
    "        (\"rocket\", \"rocket\", {\"num_kernels\": 10000, \"max_dilations_per_kernel\": 8})\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train HIVE-COTE ensemble separately for each category of \"Measure\"\n",
    "models = {}\n",
    "for measure in df[\"Measure\"].unique():\n",
    "    X_train_measure = X_train[X_train[\"Measure\"] == measure]\n",
    "    y_train_measure = y_train[X_train[\"Measure\"] == measure]\n",
    "    model = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"ensemble\", ensemble)\n",
    "    ])\n",
    "    model.fit(X_train_measure.drop(\"Measure\", axis=1), y_train_measure)\n",
    "    models[measure] = model\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = []\n",
    "for i, row in X_test.iterrows():\n",
    "    measure = row[\"Measure\"]\n",
    "    model = models[measure]\n",
    "    y_pred.append(model.predict([row.drop(\"Measure\")])[0])\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2452a24994f45300bfc17642e7a3cbaee7ec1bbf9e6ae0087a7c4fe13206dc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
